# 微博爬虫
2020年4月

可以爬取的数据：给定**时间**和**地点**和**关键词**，爬取所有搜索结果。
参考了网上很多不同的代码，发现都用不了（微博经常更新），而且很少是用来爬取给定地点、时间和关键词的搜索结果。

## 使用方法：
1. 将想要搜索的**关键词**和**省份**分别在keywords.txt和provinces.txt输入，一行一个，不得有错字或者多余符号。
2. 在main.py中设置`START_DATE`和`END_DATE`为搜索**时间范围**。
3. 在浏览器**手动登录**weibo.com，拷贝cookies并在main.py中黏贴为`COOKIES`的值。
4. 执行`$ python main.py`。

所有细节都嵌套在weibo.py中，仅需按上述步骤即可开始爬取。本程序要求手动用浏览器登录，然后通过浏览器发送的包获取cookies（欢迎你帮我搞登录，微博太难搞了）。

## 性能
一台计算机一个账号，理想情况可以一秒5条，即一分钟300条，以天432000条，当然如果你有账号池，或者会多线程，可以自己进行改进。目前每页sleep两秒，不然有captcha，欢迎你帮我弄掉微博captcha。其实1秒也不一定有captcha，但是安全起见。

**更新**：貌似现在5秒一条也是有可能被反爬虫的……微博的反爬虫机制有点玄学。

## 作者：

陈英发，Donny Chan。
最后一次更新：2020年3月28日
